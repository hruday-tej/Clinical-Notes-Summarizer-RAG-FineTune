{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8129070,"sourceType":"datasetVersion","datasetId":4804364}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n!pip install -q datasets bitsandbytes einops wandb","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-03T15:10:51.232686Z","iopub.execute_input":"2024-05-03T15:10:51.232949Z","iopub.status.idle":"2024-05-03T15:11:48.573133Z","shell.execute_reply.started":"2024-05-03T15:10:51.232924Z","shell.execute_reply":"2024-05-03T15:11:48.571792Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!python --version","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:11:48.575815Z","iopub.execute_input":"2024-05-03T15:11:48.576227Z","iopub.status.idle":"2024-05-03T15:11:49.572331Z","shell.execute_reply.started":"2024-05-03T15:11:48.576186Z","shell.execute_reply":"2024-05-03T15:11:49.571369Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Python 3.10.13\n","output_type":"stream"}]},{"cell_type":"code","source":"!huggingface-cli login --token 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","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:11:49.577768Z","iopub.execute_input":"2024-05-03T15:11:49.578055Z","iopub.status.idle":"2024-05-03T15:11:51.107507Z","shell.execute_reply.started":"2024-05-03T15:11:49.578027Z","shell.execute_reply":"2024-05-03T15:11:51.106574Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\n\n# Initialize empty lists to store data\nids = []\ndialogues = []\nsummaries = []\n\n# Open the JSONL file and read its contents line by line\nwith open(\"/kaggle/input/test-train-jsonl/train.jsonl\", \"r\") as file:\n    for line in file:\n        # Parse each JSON object in the JSONL file\n        data = json.loads(line)\n        # Extract values for 'idx', 'inputs', and 'target'\n        idx = data[\"idx\"]\n        dialogue = data[\"inputs\"]\n        summary = data[\"target\"]\n        # Append the values to respective lists\n        ids.append(idx)\n        dialogues.append(dialogue)\n        summaries.append(summary)\n\n# Create a Hugging Face dataset using the lists of data\ndataset = Dataset.from_dict({\n    \"id\": ids,\n    \"dialogue\": dialogues,\n    \"summary\": summaries\n})\ntrain_dataset = dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-03T15:11:51.108843Z","iopub.execute_input":"2024-05-03T15:11:51.109101Z","iopub.status.idle":"2024-05-03T15:11:53.056056Z","shell.execute_reply.started":"2024-05-03T15:11:51.109076Z","shell.execute_reply":"2024-05-03T15:11:53.055267Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset[0]","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-03T15:11:53.057107Z","iopub.execute_input":"2024-05-03T15:11:53.057404Z","iopub.status.idle":"2024-05-03T15:11:53.069163Z","shell.execute_reply.started":"2024-05-03T15:11:53.057370Z","shell.execute_reply":"2024-05-03T15:11:53.068272Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'id': 0,\n 'dialogue': 'The lungs are clear, and without focal air space opacity. The cardiomediastinal silhouette is normal in size and contour, and stable. There is no pneumothorax or large pleural effusion.',\n 'summary': 'No acute cardiopulmonary abnormality.'}"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training\n\nmodel_id = \"meta-llama/Llama-2-7b-chat-hf\"\n\n# \n# load model in NF4 quantization with double quantization,\n# set compute dtype to bfloat16\n# \nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    # bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    # use_cache=False,\n    # device_map=\"auto\",\n)\n# model = prepare_model_for_kbit_training(model)\nmodel.config.use_cache = False","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-03T15:11:53.070256Z","iopub.execute_input":"2024-05-03T15:11:53.070580Z","iopub.status.idle":"2024-05-03T15:13:06.091872Z","shell.execute_reply.started":"2024-05-03T15:11:53.070551Z","shell.execute_reply":"2024-05-03T15:13:06.090702Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c993963ddfb74ea4ba1002062ff2c04d"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0256441b61194e349a11755df14c91a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b95dbc71a3524c0b97e05d4e0593d64f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32a1e6f2dd374ae889206dd177acbd5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fbd7882115e42dbae7841a2ed46ca2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f904d64530eb4428a8186aeb7b3631f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"970968542fda47ee874cf51c71cca4d0"}},"metadata":{}}]},{"cell_type":"code","source":"def prompt_formatter(sample):\n    return f\"\"\"<s>### Instruction:\nYou are a helpful, respectful and honest assistant. \\\nYour task is to summarize the following dialogue. \\\nYour answer should be based on the provided dialogue only.\n\n### Dialogue:\n{sample['dialogue']}\n\n### Summary:\n{sample['summary']} </s>\"\"\"\n\nn = 0\nprint(prompt_formatter(train_dataset[n]))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-03T15:13:12.009767Z","iopub.execute_input":"2024-05-03T15:13:12.010338Z","iopub.status.idle":"2024-05-03T15:13:12.016760Z","shell.execute_reply.started":"2024-05-03T15:13:12.010307Z","shell.execute_reply":"2024-05-03T15:13:12.015845Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<s>### Instruction:\nYou are a helpful, respectful and honest assistant. Your task is to summarize the following dialogue. Your answer should be based on the provided dialogue only.\n\n### Dialogue:\nThe lungs are clear, and without focal air space opacity. The cardiomediastinal silhouette is normal in size and contour, and stable. There is no pneumothorax or large pleural effusion.\n\n### Summary:\nNo acute cardiopulmonary abnormality. </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\n\n# \n# construct a Peft model.\n# the QLoRA paper recommends LoRA dropout = 0.05 for small models (7B, 13B)\n# \npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\", \n)\nmodel = get_peft_model(model, peft_config)\n\n# \n# set up the trainer\n# \ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nargs = TrainingArguments(\n    output_dir=\"llama2-7b-chat-opr\",\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=2,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    learning_rate=2e-4,\n    optim=\"paged_adamw_32bit\",\n    bf16=False,\n    fp16=True,\n#     tf32=True,\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"constant\",\n    disable_tqdm=False,\n)\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    peft_config=peft_config,\n    max_seq_length=256,\n    tokenizer=tokenizer,\n    packing=True,\n    formatting_func=prompt_formatter, \n    args=args,\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-03T15:13:16.061494Z","iopub.execute_input":"2024-05-03T15:13:16.062243Z","iopub.status.idle":"2024-05-03T15:13:38.097600Z","shell.execute_reply.started":"2024-05-03T15:13:16.062213Z","shell.execute_reply":"2024-05-03T15:13:38.096641Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-05-03 15:13:27.099181: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-03 15:13:27.099304: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-03 15:13:27.235779: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2abf4a9ec3a346b99b171bc8973d1845"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"257d4af294e043da9717d12d75b99162"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b82cfe8e06e048fe8ffab6c83a5c10a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4b751df05dc4ad293e65198f3c02d8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc1513c5700141e59d29b9596a09a2af"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-03T15:13:38.099511Z","iopub.execute_input":"2024-05-03T15:13:38.100182Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240503_151413-ipovcjon</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hruday-solves/huggingface/runs/ipovcjon/workspace' target=\"_blank\">fresh-forest-18</a></strong> to <a href='https://wandb.ai/hruday-solves/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hruday-solves/huggingface' target=\"_blank\">https://wandb.ai/hruday-solves/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hruday-solves/huggingface/runs/ipovcjon/workspace' target=\"_blank\">https://wandb.ai/hruday-solves/huggingface/runs/ipovcjon/workspace</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='361' max='365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [361/365 30:51 < 00:20, 0.19 it/s, Epoch 0.98/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.302000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.755600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.323900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.107800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.088900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.037000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.045900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.940600</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.931200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.988200</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.842600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.737400</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.753800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.710900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.727100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.710400</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.854700</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.734200</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.639000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.663100</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.710300</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.666700</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.709000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.674000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.640400</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.700400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.641600</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.670500</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.714800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.692900</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.682600</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.650200</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.581200</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.654500</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.675800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"output_folder = \"llama2-7b-chat-opr-save\"\nmodel.save_pretrained(output_folder, safe_serialization=True)\n\ntrainer.save_model()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-03T16:13:30.479706Z","iopub.execute_input":"2024-05-03T16:13:30.480491Z","iopub.status.idle":"2024-05-03T16:13:31.115452Z","shell.execute_reply.started":"2024-05-03T16:13:30.480456Z","shell.execute_reply":"2024-05-03T16:13:31.114332Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_folder = \"llama2-7b-chat-opr/\"\n\n# load both the adapter and the base model\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    model_folder,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n    device_map='auto'\n)\ntokenizer = AutoTokenizer.from_pretrained(model_folder)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-03T16:13:44.123467Z","iopub.execute_input":"2024-05-03T16:13:44.123891Z","iopub.status.idle":"2024-05-03T16:13:57.068525Z","shell.execute_reply.started":"2024-05-03T16:13:44.123858Z","shell.execute_reply":"2024-05-03T16:13:57.067192Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9069e0d040f42078d1757ecc4fedc0b"}},"metadata":{}}]},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\n\n# Initialize empty lists to store data\nids = []\ndialogues = []\nsummaries = []\n\n# Open the JSONL file and read its contents line by line\nwith open(\"/kaggle/input/test-train-jsonl/test.jsonl\", \"r\") as file:\n    for line in file:\n        # Parse each JSON object in the JSONL file\n        data = json.loads(line)\n        # Extract values for 'idx', 'inputs', and 'target'\n        idx = data[\"idx\"]\n        dialogue = data[\"inputs\"]\n        summary = data[\"target\"]\n        # Append the values to respective lists\n        ids.append(idx)\n        dialogues.append(dialogue)\n        summaries.append(summary)\n\n# Create a Hugging Face dataset using the lists of data\ntest_dataset = Dataset.from_dict({\n    \"id\": ids,\n    \"dialogue\": dialogues,\n    \"summary\": summaries\n})","metadata":{"execution":{"iopub.status.busy":"2024-05-03T16:14:07.042738Z","iopub.execute_input":"2024-05-03T16:14:07.043434Z","iopub.status.idle":"2024-05-03T16:14:07.076709Z","shell.execute_reply.started":"2024-05-03T16:14:07.043403Z","shell.execute_reply":"2024-05-03T16:14:07.075749Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"sample = test_dataset[201]\n\nprompt = f\"\"\"### Instruction:\nYou are a helpful, respectful and honest assistant. \\\nYour task is to summarize the following dialogue. \\\nYour answer should be based on the provided dialogue only.\n\n### Dialogue:\n{sample['dialogue']}\n\n### Summary:\n\"\"\"\nprint(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T16:15:04.910785Z","iopub.execute_input":"2024-05-03T16:15:04.911158Z","iopub.status.idle":"2024-05-03T16:15:04.918650Z","shell.execute_reply.started":"2024-05-03T16:15:04.911128Z","shell.execute_reply":"2024-05-03T16:15:04.917701Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"### Instruction:\nYou are a helpful, respectful and honest assistant. Your task is to summarize the following dialogue. Your answer should be based on the provided dialogue only.\n\n### Dialogue:\nHeart size normal. Lungs are clear. XXXX are normal. No pneumonia, effusions, edema, pneumothorax, adenopathy, nodules or masses.\n\n### Summary:\n\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\noutputs = model.generate(input_ids=input_ids, max_new_tokens=50, temperature=0.7)\n\nprint('Output:\\n',\n      tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):])\nprint('\\nGround truth:\\n', sample['summary'])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T16:15:05.848705Z","iopub.execute_input":"2024-05-03T16:15:05.849363Z","iopub.status.idle":"2024-05-03T16:15:07.303053Z","shell.execute_reply.started":"2024-05-03T16:15:05.849320Z","shell.execute_reply":"2024-05-03T16:15:07.301776Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Output:\n No acute cardiopulmonary abnormalities \n\nGround truth:\n Normal chest.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom peft import AutoPeftModelForCausalLM\n\nmodel_folder = \"llama2-7b-chat-opr\"\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    model_folder,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16\n)\n\n# merge the lora adapter and the base model\nmerged_model = model.merge_and_unload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\noutput_folder = 'merged-llama2-7b-chat-opr-blank'\n\n# save the merged model and the tokenizer\nmerged_model.save_pretrained(output_folder, safe_serialization=True)\n\n# tokenizer = AutoTokenizer.from_pretrained(model_folder)\n# tokenizer.save_pretrained(output_folder)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear output folder\nimport os\n\ndef remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)\n\nfolder_path = '/kaggle/working/merged-llama2-7b-chat-opr-blank'\nremove_folder_contents(folder_path)\nos.rmdir(folder_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_folder = 'merged-llama2-7b-chat-opr'\n\ntokenizer = AutoTokenizer.from_pretrained(model_folder)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_folder,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:08:43.440284Z","iopub.execute_input":"2024-04-15T19:08:43.441080Z","iopub.status.idle":"2024-04-15T19:09:50.314682Z","shell.execute_reply.started":"2024-04-15T19:08:43.441046Z","shell.execute_reply":"2024-04-15T19:09:50.313705Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"915fe5f82ac94ae2974144b7d58c684c"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline, GenerationConfig\n\ngen_config = GenerationConfig.from_pretrained(model_folder)\ngen_config.max_new_tokens = 50\ngen_config.temperature = 0.7\ngen_config.repetition_penalty = 1.1\ngen_config.pad_token_id = tokenizer.eos_token_id\n\npipe = pipeline(\n    task=\"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map='auto',\n    generation_config=gen_config,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:10:17.303712Z","iopub.execute_input":"2024-04-15T19:10:17.304106Z","iopub.status.idle":"2024-04-15T19:10:17.387842Z","shell.execute_reply.started":"2024-04-15T19:10:17.304076Z","shell.execute_reply":"2024-04-15T19:10:17.386887Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"sample = test_dataset[44]\n\nprompt = f\"\"\"### Instruction:\nYou are a helpful, respectful and honest assistant. \\\nYour task is to summarize the following dialogue. \\\nYour answer should be based on the provided dialogue only.\n\n### Dialogue:\n{sample['dialogue']}\n\n### Summary:\n\"\"\"\n\noutput = pipe(prompt)\n\nprint('Output:\\n', output[0]['generated_text'][len(prompt):])\nprint('\\nGround truth:\\n', sample['summary'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = \"test/saved\"\nmodel.save_pretrained(output_dir, safe_serialization=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'/kaggle/working/llama2-7b-chat-opr')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom zipfile import ZipFile\n\n# Define the directory containing the files\ndirectory = '/kaggle/working/merged-llama2-7b-chat-opr'\n\n# Create a list of files in the directory\nfiles = os.listdir(directory)\n\n# Create a ZIP archive containing all the files\nwith ZipFile('/kaggle/working/merged_files.zip', 'w') as zipf:\n    for file in files:\n        # Add each file to the ZIP archive\n        zipf.write(os.path.join(directory, file), file)\n\n# Provide a download link for the ZIP archive\nfrom IPython.display import FileLink\nFileLink(r'/kaggle/working/merged_files.zip')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/merged_files.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nimport os\nfrom IPython.display import FileLink\n\ndef zip_dir(directory = \"/kaggle/working/llama2-7b-chat-opr-blank\", file_name = 'outputs.zip'):\n    \"\"\"\n    zip all the files in a directory\n    \n    Parameters\n    _____\n    directory: str\n        directory needs to be zipped, defualt is current working directory\n        \n    file_name: str\n        the name of the zipped file (including .zip), default is 'directory.zip'\n        \n    Returns\n    _____\n    Creates a hyperlink, which can be used to download the zip file)\n    \"\"\"\n    os.chdir(directory)\n    zip_ref = zipfile.ZipFile(file_name, mode='w')\n    for folder, _, files in os.walk(directory):\n        for file in files:\n            if file_name in file:\n                pass\n            else:\n                zip_ref.write(os.path.join(folder, file))\n\n    return FileLink(file_name)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T17:28:44.472537Z","iopub.execute_input":"2024-04-19T17:28:44.473456Z","iopub.status.idle":"2024-04-19T17:28:44.482247Z","shell.execute_reply.started":"2024-04-19T17:28:44.473411Z","shell.execute_reply":"2024-04-19T17:28:44.481089Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"zip_dir()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T17:28:50.511823Z","iopub.execute_input":"2024-04-19T17:28:50.512193Z","iopub.status.idle":"2024-04-19T17:28:50.564461Z","shell.execute_reply.started":"2024-04-19T17:28:50.512162Z","shell.execute_reply":"2024-04-19T17:28:50.563530Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/llama2-7b-chat-opr-blank/outputs.zip","text/html":"<a href='outputs.zip' target='_blank'>outputs.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install bert-score","metadata":{"execution":{"iopub.status.busy":"2024-04-19T17:30:09.707504Z","iopub.execute_input":"2024-04-19T17:30:09.707911Z","iopub.status.idle":"2024-04-19T17:30:22.198527Z","shell.execute_reply.started":"2024-04-19T17:30:09.707881Z","shell.execute_reply":"2024-04-19T17:30:22.197434Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.1.2)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.1.4)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.40.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.31.0)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.66.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert-score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert-score) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2023.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.22.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.4.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (2024.2.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert-score\nSuccessfully installed bert-score-0.3.13\n","output_type":"stream"}]},{"cell_type":"code","source":"# from bert_score import score\n\n# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n# outputs = model.generate(input_ids=input_ids, max_new_tokens=50, temperature=0.7, )\n\n# print('Output:\\n',\n#       tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):])\n# print('\\nGround truth:\\n', sample['summary'])\n\n                       \n# print(score(tokenizer.batch_decode(outputs.detach().cpu().numpy(), sample[\"summary\"], lang=\"en\", verbose=True)))\n\nsample = test_dataset[100]\n\nprompt = f\"\"\"### Instruction:\nYou are a helpful, respectful and honest assistant. \\\nYour task is to summarize the following dialogue. \\\nYour answer should be based on the provided dialogue only.\n\n### Dialogue:\n{sample['dialogue']}\n\n### Summary:\n\"\"\"\nprint(prompt)\n\nfrom bert_score import score\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\noutputs = model.generate(input_ids=input_ids, max_new_tokens=50, temperature=0.7)\n\npredictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\nprint('Output:\\n', predictions[0])\nprint('\\nGround truth:\\n', sample['summary'])\n\nP, R, F1 = score([predictions[0]], [sample['summary']], lang=\"en\", verbose=True)\n\nprint(f\"Precision: {P.mean():.2f}\")\nprint(f\"Recall: {R.mean():.2f}\")\nprint(f\"F1-Score: {F1.mean():.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T17:36:22.975391Z","iopub.execute_input":"2024-04-19T17:36:22.976154Z","iopub.status.idle":"2024-04-19T17:36:26.224028Z","shell.execute_reply.started":"2024-04-19T17:36:22.976121Z","shell.execute_reply":"2024-04-19T17:36:26.222987Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"### Instruction:\nYou are a helpful, respectful and honest assistant. Your task is to summarize the following dialogue. Your answer should be based on the provided dialogue only.\n\n### Dialogue:\nThe heart size is normal. The mediastinal contour is within normal limits. The lungs are free of any focal infiltrates. There are no nodules or masses. No visible pneumothorax. No visible pleural fluid. The XXXX are grossly normal. There is no visible free intraperitoneal air under the diaphragm.\n\n### Summary:\n\nOutput:\n ### Instruction:\nYou are a helpful, respectful and honest assistant. Your task is to summarize the following dialogue. Your answer should be based on the provided dialogue only.\n\n### Dialogue:\nThe heart size is normal. The mediastinal contour is within normal limits. The lungs are free of any focal infiltrates. There are no nodules or masses. No visible pneumothorax. No visible pleural fluid. The XXXX are grossly normal. There is no visible free intraperitoneal air under the diaphragm.\n\n### Summary:\n1. No evidence of active disease. \n\nGround truth:\n 1. No acute radiographic cardiopulmonary process.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b0b41f168543829a291bbcaf838b17"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc98629f11fe4fde81f1b81c884e13c1"}},"metadata":{}},{"name":"stdout","text":"done in 0.06 seconds, 15.59 sentences/sec\nPrecision: 0.80\nRecall: 0.86\nF1-Score: 0.83\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}