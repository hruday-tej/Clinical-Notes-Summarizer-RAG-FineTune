{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289db6fc-8743-4bb8-8e9f-b4f03e641961",
   "metadata": {},
   "source": [
    "# LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afa7b7-f1f4-4c76-84fa-045050f04461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f235d-47eb-42c2-b211-86a437172469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change this to the path of the opi directory\n",
    "data_path = \"./clin-summ/data/opi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747e3ad-6376-4a53-892b-13bec9cf4999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "assert os.path.exists(data_path), 'Dataset not found ({})'.format(data_path)\n",
    "\n",
    "def load_data(filename: str):\n",
    "    df = pd.read_json(os.path.join(data_path, filename), lines=True)\n",
    "    return df[\"inputs\"].to_numpy(), df[\"target\"].to_numpy()\n",
    "\n",
    "train_x, train_y = load_data(\"train.jsonl\")\n",
    "val_x, val_y = load_data(\"validate.jsonl\")\n",
    "test_x, test_y = load_data(\"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507889b2-0430-4c14-afff-912684dcf999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add start and end tokens to summaries\n",
    "train_y = ['sostok ' + y + ' eostok' for y in train_y]\n",
    "val_y = ['sostok ' + y + ' eostok' for y in val_y]\n",
    "test_y = ['sostok ' + y + ' eostok' for y in test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af33c4-3084-4a0b-a55c-2109e6319245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_lengths = [len(text.split()) for text in train_x]\n",
    "summary_lengths = [len(summary.split()) for summary in train_y]\n",
    "\n",
    "# Show the distribution of text/summary lengths\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "counts_x, bins_x = np.histogram(text_lengths)\n",
    "ax1.stairs(counts_x, bins_x)\n",
    "counts_y, bins_y = np.histogram(summary_lengths)\n",
    "ax2.stairs(counts_y, bins_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ea599-b158-4395-b94b-056a23da0972",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_text_len = np.max(text_lengths)\n",
    "max_summary_len = np.max(summary_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb99e2-9595-4b7c-b62f-7dd53679ad8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "# Tokenize text and summaries, and pad to maximum length.\n",
    "\n",
    "max_tokens = 5000\n",
    "x_tokenizer = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode='int')\n",
    "x_tokenizer.adapt(train_x)\n",
    "\n",
    "train_x_seq = x_tokenizer(train_x)\n",
    "val_x_seq = x_tokenizer(val_x)\n",
    "test_x_seq = x_tokenizer(test_x)\n",
    "\n",
    "y_tokenizer = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode='int')\n",
    "y_tokenizer.adapt(train_y)\n",
    "\n",
    "train_y_seq = y_tokenizer(train_y)\n",
    "val_y_seq = y_tokenizer(val_y)\n",
    "test_y_seq = y_tokenizer(test_y)\n",
    "\n",
    "# Pad text/summaries to max length by adding 0s at the end.\n",
    "encoder_input_tr = pad_sequences(train_x_seq, maxlen=max_text_len, padding=\"post\")\n",
    "encoder_input_val = pad_sequences(val_x_seq, maxlen=max_text_len, padding=\"post\")\n",
    "encoder_input_test = pad_sequences(test_x_seq, maxlen=max_text_len, padding=\"post\")\n",
    "\n",
    "decoder_input_tr = pad_sequences(train_y_seq, maxlen=max_summary_len, padding=\"post\")\n",
    "decoder_input_val = pad_sequences(val_y_seq, maxlen=max_summary_len, padding=\"post\")\n",
    "decoder_input_test = pad_sequences(test_y_seq, maxlen=max_summary_len, padding=\"post\")\n",
    "\n",
    "# Move sequence back one step for teacher forcing.\n",
    "decoder_target_tr = np.roll(decoder_input_tr, -1, axis=1)\n",
    "decoder_target_tr[:, -1] = 0 \n",
    "decoder_target_val = np.roll(decoder_input_val, -1, axis=1)\n",
    "decoder_target_val[:, -1] = 0 \n",
    "decoder_target_test = np.roll(decoder_input_test, -1, axis=1)\n",
    "decoder_target_test[:, -1] = 0 \n",
    "\n",
    "x_voc = len(x_tokenizer.get_vocabulary()) + 1\n",
    "y_voc = len(y_tokenizer.get_vocabulary()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab672aab-c2f8-41c1-97c9-da49ec2820ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(encoder_input_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13df28-a111-4e7d-b252-17d0e54314b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://keras.io/examples/nlp/lstm_seq2seq/\n",
    "latent_dim = 300\n",
    "embedding_dim = 200\n",
    "\n",
    "# Encoder Input\n",
    "encoder_inputs = Input(shape=(max_text_len,), name='enc_input')\n",
    "\n",
    "# Embedding layer\n",
    "enc_emb =  Embedding(x_voc, max_text_len, trainable=True, name='enc_embedding')(encoder_inputs)\n",
    "\n",
    "# Encoder LSTMs\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.5, recurrent_dropout=0.5, name='enc_lstm1')\n",
    "encoder_outputs1, state_h1, state_c1= encoder_lstm1(enc_emb)\n",
    "\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.5, recurrent_dropout=0.5, name='enc_lstm2')\n",
    "encoder_outputs2, state_h2, state_c2= encoder_lstm2(encoder_outputs1)\n",
    "\n",
    "encoder_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.5, recurrent_dropout=0.5, name='enc_lstm3')\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_outputs2)\n",
    "\n",
    "decoder_inputs = Input(shape=(None,), name='dec_input')\n",
    "\n",
    "# Embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True, name='dec_embedding')\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.5, recurrent_dropout=0.25, name='dec_lstm')\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "#dense layer - softmax\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'), name='dec_output')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='lstm_baseline')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bcd91f-763c-4ccc-955a-bff80c8c2d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lstm_model = 'lstm_model.keras'\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "if os.path.exists(lstm_model):\n",
    "    model = keras.models.load_model(lstm_model)\n",
    "\n",
    "else:\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    early_stop_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    batch_size = 40\n",
    "    epochs = 20\n",
    "\n",
    "    model.fit([encoder_input_tr, decoder_input_tr],\n",
    "              decoder_target_tr,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=([encoder_input_val, decoder_input_val], decoder_target_val),\n",
    "              callbacks=[early_stop_cb]\n",
    "             )\n",
    "\n",
    "    model.save(\"lstm_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b514123-7b22-482b-8a47-c596b7abdbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reverse_encode_seq = dict((index, value) for (index, value) in enumerate(x_tokenizer.get_vocabulary()))\n",
    "reverse_decode_seq = dict((index, value) for (index, value) in enumerate(y_tokenizer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae80ea4-4f88-4319-9336-33b8849340f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c970a-0f29-493a-a3b8-320b42855601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rebuild the model for inference\n",
    "for layer in model.layers:\n",
    "    print(layer.name)\n",
    "\n",
    "model = keras.models.load_model(\"lstm_model.keras\")\n",
    "encoder_inputs = model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[6].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]\n",
    "decoder_emb = model.layers[5](decoder_inputs)\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[7]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(decoder_emb, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[8]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "def decode_sequence(input_sequence, temp=1.0):\n",
    "    states_value = encoder_model.predict(input_sequence, verbose=0)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = y_tokenizer('sostok')\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, verbose=0\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = sample(output_tokens[0, -1, :], temp)\n",
    "        sampled_vocab = reverse_decode_seq[sampled_token_index]\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if(sampled_vocab != 'eostok' and len(decoded_sentence) < 200):\n",
    "            decoded_sentence += ' ' + sampled_vocab\n",
    "        else:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74835dcc-cfe0-4c1d-be32-6c4eb174a8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take sequences from training set to test inference.\n",
    "for seq_index in range(0, 20):\n",
    "    input_seq = encoder_input_tr[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq, temp=0.7)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", train_x[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)\n",
    "    print(\"Expected:\", ' '.join(train_y[seq_index].split()[1:-1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
